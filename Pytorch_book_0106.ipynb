{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1월6일.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfASF7xlfDQUFDcOEtCVls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minh-A/Pytorch_DeepLearning/blob/main/Pytorch_book_0106.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVpR6PIxqGoZ"
      },
      "source": [
        "# 1월 6일: 다층퍼셉트론"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TthsXDrkeqK0"
      },
      "source": [
        "# 파이토치 라이브러리 임포트\r\n",
        "import torch\r\n",
        "from torch.autograd import Variable\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import DataLoader, TensorDataset\r\n",
        "\r\n",
        "# sklearn 라이브러리 임포트\r\n",
        "from sklearn.datasets import load_wine\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# pandas 라이브러리 임포트\r\n",
        "import pandas as pd"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO01f3JnjYUF",
        "outputId": "dd57719a-4872-4d33-9022-8e4a597bf945"
      },
      "source": [
        "wine = load_wine()\r\n",
        "wine"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 178 (50 in each of three classes)\\n    :Number of Attributes: 13 numeric, predictive attributes and the class\\n    :Attribute Information:\\n \\t\\t- Alcohol\\n \\t\\t- Malic acid\\n \\t\\t- Ash\\n\\t\\t- Alcalinity of ash  \\n \\t\\t- Magnesium\\n\\t\\t- Total phenols\\n \\t\\t- Flavanoids\\n \\t\\t- Nonflavanoid phenols\\n \\t\\t- Proanthocyanins\\n\\t\\t- Color intensity\\n \\t\\t- Hue\\n \\t\\t- OD280/OD315 of diluted wines\\n \\t\\t- Proline\\n\\n    - class:\\n            - class_0\\n            - class_1\\n            - class_2\\n\\t\\t\\n    :Summary Statistics:\\n    \\n    ============================= ==== ===== ======= =====\\n                                   Min   Max   Mean     SD\\n    ============================= ==== ===== ======= =====\\n    Alcohol:                      11.0  14.8    13.0   0.8\\n    Malic Acid:                   0.74  5.80    2.34  1.12\\n    Ash:                          1.36  3.23    2.36  0.27\\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\\n    Magnesium:                    70.0 162.0    99.7  14.3\\n    Total Phenols:                0.98  3.88    2.29  0.63\\n    Flavanoids:                   0.34  5.08    2.03  1.00\\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\\n    Proanthocyanins:              0.41  3.58    1.59  0.57\\n    Colour Intensity:              1.3  13.0     5.1   2.3\\n    Hue:                          0.48  1.71    0.96  0.23\\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\n    Proline:                       278  1680     746   315\\n    ============================= ==== ===== ======= =====\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners: \\n\\nForina, M. et al, PARVUS - \\nAn Extendible Package for Data Exploration, Classification and Correlation. \\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science. \\n\\n.. topic:: References\\n\\n  (1) S. Aeberhard, D. Coomans and O. de Vel, \\n  Comparison of Classifiers in High Dimensional Settings, \\n  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Technometrics). \\n\\n  The data was used with many others for comparing various \\n  classifiers. The classes are separable, though only RDA \\n  has achieved 100% correct classification. \\n  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \\n  (All results using the leave-one-out technique) \\n\\n  (2) S. Aeberhard, D. Coomans and O. de Vel, \\n  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \\n  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Journal of Chemometrics).\\n',\n",
              " 'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
              "         1.065e+03],\n",
              "        [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
              "         1.050e+03],\n",
              "        [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
              "         1.185e+03],\n",
              "        ...,\n",
              "        [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
              "         8.350e+02],\n",
              "        [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
              "         8.400e+02],\n",
              "        [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
              "         5.600e+02]]),\n",
              " 'feature_names': ['alcohol',\n",
              "  'malic_acid',\n",
              "  'ash',\n",
              "  'alcalinity_of_ash',\n",
              "  'magnesium',\n",
              "  'total_phenols',\n",
              "  'flavanoids',\n",
              "  'nonflavanoid_phenols',\n",
              "  'proanthocyanins',\n",
              "  'color_intensity',\n",
              "  'hue',\n",
              "  'od280/od315_of_diluted_wines',\n",
              "  'proline'],\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2]),\n",
              " 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='<U7')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "tJ5xd0gJjYYs",
        "outputId": "87c08843-4c1d-48cc-d90b-44f3e5570699"
      },
      "source": [
        "pd.DataFrame(wine.data, columns= wine.feature_names)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96.0</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     alcohol  malic_acid   ash  ...   hue  od280/od315_of_diluted_wines  proline\n",
              "0      14.23        1.71  2.43  ...  1.04                          3.92   1065.0\n",
              "1      13.20        1.78  2.14  ...  1.05                          3.40   1050.0\n",
              "2      13.16        2.36  2.67  ...  1.03                          3.17   1185.0\n",
              "3      14.37        1.95  2.50  ...  0.86                          3.45   1480.0\n",
              "4      13.24        2.59  2.87  ...  1.04                          2.93    735.0\n",
              "..       ...         ...   ...  ...   ...                           ...      ...\n",
              "173    13.71        5.65  2.45  ...  0.64                          1.74    740.0\n",
              "174    13.40        3.91  2.48  ...  0.70                          1.56    750.0\n",
              "175    13.27        4.28  2.26  ...  0.59                          1.56    835.0\n",
              "176    13.17        2.59  2.37  ...  0.60                          1.62    840.0\n",
              "177    14.13        4.10  2.74  ...  0.61                          1.60    560.0\n",
              "\n",
              "[178 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrgJW3cTjYbR",
        "outputId": "a82c345a-6a3d-44f8-820e-c057e4d3938d"
      },
      "source": [
        "wine.target"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9ObSk3CjYdb"
      },
      "source": [
        "wine_data = wine.data[0:130]\r\n",
        "wine_target = wine.target[0:130]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa-FahwtjYgB",
        "outputId": "b36ccd84-ec3b-406b-a664-0dec29062fc9"
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(wine_data, wine_target, test_size = 0.2)\r\n",
        "\r\n",
        "print(len(train_x))\r\n",
        "print(len(test_x))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "104\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fngus1IWjYia",
        "outputId": "7ed3cbe0-616e-4cf3-8eb3-dc00f4775d8b"
      },
      "source": [
        "train_x = torch.from_numpy(train_x).float()\r\n",
        "train_y = torch.from_numpy(train_y).long()\r\n",
        "\r\n",
        "test_x = torch.from_numpy(test_x).float()\r\n",
        "test_y = torch.from_numpy(test_y).long()\r\n",
        "\r\n",
        "print(train_x.shape)\r\n",
        "print(train_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([104, 13])\n",
            "torch.Size([104])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J61blhY_Atr7"
      },
      "source": [
        "> Numpy 배열을 텐서로 변환\r\n",
        "* torch.from_numpy()\r\n",
        "\r\n",
        "> 변수들의 합쳐서 하나의 데이터 집합을 생성\r\n",
        "* TensorDataset()\r\n",
        "\r\n",
        "> 데이터 집합을 미니배치로 나누어\r\n",
        "* DataLoader()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQcXNhkmjYnb",
        "outputId": "e7d21e7c-a5bb-40a4-e2bb-34e788548813"
      },
      "source": [
        "train = TensorDataset(train_x, train_y)\r\n",
        "\r\n",
        "print(train[0])\r\n",
        "\r\n",
        "# 미니배치로 분할\r\n",
        "train_loader = DataLoader(train, batch_size=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([1.2040e+01, 4.3000e+00, 2.3800e+00, 2.2000e+01, 8.0000e+01, 2.1000e+00,\n",
            "        1.7500e+00, 4.2000e-01, 1.3500e+00, 2.6000e+00, 7.9000e-01, 2.5700e+00,\n",
            "        5.8000e+02]), tensor(1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4usu7ppQHWqF",
        "outputId": "94b893b7-29c0-4548-b411-03f3a1cf9bb8"
      },
      "source": [
        "for i,b in train_loader:\r\n",
        "  print(b)\r\n",
        "\r\n",
        "  # 임마가 배치사이즈 16개\r\n",
        "  # 전체 텐서는 7개"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
            "tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0])\n",
            "tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0])\n",
            "tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0])\n",
            "tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0])\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYmquNv8jYp0"
      },
      "source": [
        "# 신경망 구성\r\n",
        "class Net(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(Net,self).__init__()\r\n",
        "    self.fc1 = nn.Linear(13,96)\r\n",
        "    self.fc2 = nn.Linear(96,96)\r\n",
        "    self.fc6 = nn.Linear(96,2)\r\n",
        "\r\n",
        "  def forward(self,x):\r\n",
        "    x = F.relu(self.fc1(x))\r\n",
        "    x = F.relu(self.fc2(x))\r\n",
        "    x = F.relu(self.fc2(x))\r\n",
        "    x = F.relu(self.fc2(x))\r\n",
        "    x = F.relu(self.fc2(x))\r\n",
        "    x = F.relu(self.fc2(x))\r\n",
        "    x = self.fc6(x)\r\n",
        "    return F.log_softmax(x)\r\n",
        "\r\n",
        "# 인스턴스 생성\r\n",
        "model = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrat5SaOJewY"
      },
      "source": [
        "> 초기 모듈을 생성\r\n",
        "* nn.Modeule\r\n",
        "\r\n",
        "> 층 (선형변환)\r\n",
        "* nn.Linear()    여기에 bias도 들어간다잉\r\n",
        "\r\n",
        "> 활성화 함수 (앞의 F는 functional의 약자)\r\n",
        "* F.relu()     : Relu함수\r\n",
        "* F.log_softmax   : 로그소프트맥스 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Vzs220bp-i"
      },
      "source": [
        "> 크로스엔트로피 설명\r\n",
        "* https://hyunw.kim/blog/2017/10/26/Cross_Entropy.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s0dJGU5jYsa",
        "outputId": "8f5696f9-98f9-45ee-9297-ac10d1132e0a"
      },
      "source": [
        "# 모형 학습\r\n",
        "\r\n",
        "# 오차함수 객체\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# 최적화 담당\r\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)  #경사하강법은 SGD\r\n",
        "\r\n",
        "# 학습시작\r\n",
        "for epoch in range(300):\r\n",
        "  total_loss = 0\r\n",
        "\r\n",
        "  for train_x, train_y in train_loader:\r\n",
        "    train_x, train_y = Variable(train_x), Variable(train_y)  #이거만다 사용하너\r\n",
        "\r\n",
        "    optimizer.zero_grad()  # 경사초기화\r\n",
        "\r\n",
        "    output = model(train_x)  # 순전파 계산\r\n",
        "\r\n",
        "    loss = criterion(output, train_y)  # 오차 계산\r\n",
        "    loss.backward()  # 역전파 계산\r\n",
        "\r\n",
        "    optimizer.step()  #가중치 업데이트\r\n",
        "\r\n",
        "    total_loss += loss.data  # 누적오차 계산\r\n",
        "  \r\n",
        "  if (epoch+1) % 50 == 0:\r\n",
        "    print(epoch+1, float(total_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50 2.1234989166259766\n",
            "100 2.3042960166931152\n",
            "150 1.5479601621627808\n",
            "200 2.2357945442199707\n",
            "250 1.38120698928833\n",
            "300 1.407052755355835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t651OyJOjU8-"
      },
      "source": [
        "> 오차 함수\r\n",
        "* nn.CrossEntropyLoss()  # 크로스엔트로피\r\n",
        "\r\n",
        "> 경사하강법으로 최적화\r\n",
        "* optim.SGD(model.parameters(), lr = 0.01)\r\n",
        "\r\n",
        "> 래핑과 계산과정 기록 -- 먼데이거\r\n",
        "* Variable()\r\n",
        "\r\n",
        "> 경사의 합을 구함\r\n",
        "* backward()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1s-fi1j3BKe",
        "outputId": "e02c5318-25f6-4521-f825-9a2abe53b466"
      },
      "source": [
        "test_x, test_y = Variable(test_x), Variable(test_y)   # 이거 안해도 문제 없움\r\n",
        "\r\n",
        "# 출력이 0또는 1이 되도록\r\n",
        "result = torch.max(model(test_x).data, 1)[1]\r\n",
        "\r\n",
        "# 측정\r\n",
        "accuracy = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\r\n",
        "accuracy\r\n",
        "\r\n",
        "# torch.max()는 최댓값을 반환하여 값을 매기는데, 이게 두개의 열이라면 첫 열이 크면 0을 나타내고, 두번째 열이 크면 1을 나타냄(그 위치?를 나타낸다고 볼 수 있우)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8846153846153846"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p96H2aIBnbz1"
      },
      "source": [
        "> 최댓값 반환\r\n",
        "* torch.max()\r\n",
        "* 그 위치를 반환한다고 할 수 있또(아래 두 코드창을 보삼!!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "L6JnK3nRZDi1",
        "outputId": "8f801fd4-40b0-4985-c4ae-78d1ae9d1e59"
      },
      "source": [
        "pd.DataFrame(torch.max(model(test_x).data,1))  \r\n",
        "# torch.max()안에 0과1은 행과 열로 큰 것을 나타내는 것임!!!\r\n",
        "# 0은 행의 순서 중에서 제일 큰것을 나타냄 (그러니까 열의 개수만큼 나옴)\r\n",
        "# 1은 열의 순서 중에서 제일 큰것을 나타냄 (행의 개수만큼 나옴)\r\n",
        "# 조금 헷갈리지만 직접해보삼 ㅋㅋㅋ"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tensor(-0.0015)</td>\n",
              "      <td>tensor(-0.0309)</td>\n",
              "      <td>tensor(-1.7762e-05)</td>\n",
              "      <td>tensor(-0.6908)</td>\n",
              "      <td>tensor(-0.0019)</td>\n",
              "      <td>tensor(-0.0018)</td>\n",
              "      <td>tensor(-0.5185)</td>\n",
              "      <td>tensor(-0.0032)</td>\n",
              "      <td>tensor(-0.2902)</td>\n",
              "      <td>tensor(-0.0009)</td>\n",
              "      <td>tensor(-0.0002)</td>\n",
              "      <td>tensor(-0.0081)</td>\n",
              "      <td>tensor(-0.2040)</td>\n",
              "      <td>tensor(-0.3346)</td>\n",
              "      <td>tensor(-0.0003)</td>\n",
              "      <td>tensor(-0.0054)</td>\n",
              "      <td>tensor(-0.0010)</td>\n",
              "      <td>tensor(-0.6745)</td>\n",
              "      <td>tensor(-0.0137)</td>\n",
              "      <td>tensor(-0.0028)</td>\n",
              "      <td>tensor(-0.0017)</td>\n",
              "      <td>tensor(-0.0094)</td>\n",
              "      <td>tensor(-0.0003)</td>\n",
              "      <td>tensor(-0.0015)</td>\n",
              "      <td>tensor(-0.0165)</td>\n",
              "      <td>tensor(-0.3060)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(1)</td>\n",
              "      <td>tensor(0)</td>\n",
              "      <td>tensor(0)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0                1   ...               24               25\n",
              "0  tensor(-0.0015)  tensor(-0.0309)  ...  tensor(-0.0165)  tensor(-0.3060)\n",
              "1        tensor(1)        tensor(0)  ...        tensor(0)        tensor(0)\n",
              "\n",
              "[2 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "un6qzzj73BH0",
        "outputId": "66327355-f2f0-4c3f-89ac-f9b10e9ba5a2"
      },
      "source": [
        "pd.DataFrame(model(test_x).data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tensor(-6.4935)</td>\n",
              "      <td>tensor(-0.0015)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tensor(-0.0309)</td>\n",
              "      <td>tensor(-3.4927)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tensor(-1.7762e-05)</td>\n",
              "      <td>tensor(-10.9375)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tensor(-0.6908)</td>\n",
              "      <td>tensor(-0.6955)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tensor(-6.2532)</td>\n",
              "      <td>tensor(-0.0019)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tensor(-0.0018)</td>\n",
              "      <td>tensor(-6.2953)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>tensor(-0.9049)</td>\n",
              "      <td>tensor(-0.5185)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tensor(-5.7442)</td>\n",
              "      <td>tensor(-0.0032)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tensor(-1.3789)</td>\n",
              "      <td>tensor(-0.2902)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>tensor(-0.0009)</td>\n",
              "      <td>tensor(-7.0173)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>tensor(-0.0002)</td>\n",
              "      <td>tensor(-8.7272)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>tensor(-0.0081)</td>\n",
              "      <td>tensor(-4.8145)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>tensor(-1.6897)</td>\n",
              "      <td>tensor(-0.2040)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>tensor(-0.3346)</td>\n",
              "      <td>tensor(-1.2574)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>tensor(-8.1287)</td>\n",
              "      <td>tensor(-0.0003)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>tensor(-0.0054)</td>\n",
              "      <td>tensor(-5.2233)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>tensor(-0.0010)</td>\n",
              "      <td>tensor(-6.9281)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>tensor(-0.7121)</td>\n",
              "      <td>tensor(-0.6745)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>tensor(-0.0137)</td>\n",
              "      <td>tensor(-4.2996)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>tensor(-5.8647)</td>\n",
              "      <td>tensor(-0.0028)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>tensor(-6.3836)</td>\n",
              "      <td>tensor(-0.0017)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>tensor(-0.0094)</td>\n",
              "      <td>tensor(-4.6732)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>tensor(-0.0003)</td>\n",
              "      <td>tensor(-8.0771)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>tensor(-6.5018)</td>\n",
              "      <td>tensor(-0.0015)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>tensor(-0.0165)</td>\n",
              "      <td>tensor(-4.1130)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>tensor(-0.3060)</td>\n",
              "      <td>tensor(-1.3334)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      0                 1\n",
              "0       tensor(-6.4935)   tensor(-0.0015)\n",
              "1       tensor(-0.0309)   tensor(-3.4927)\n",
              "2   tensor(-1.7762e-05)  tensor(-10.9375)\n",
              "3       tensor(-0.6908)   tensor(-0.6955)\n",
              "4       tensor(-6.2532)   tensor(-0.0019)\n",
              "5       tensor(-0.0018)   tensor(-6.2953)\n",
              "6       tensor(-0.9049)   tensor(-0.5185)\n",
              "7       tensor(-5.7442)   tensor(-0.0032)\n",
              "8       tensor(-1.3789)   tensor(-0.2902)\n",
              "9       tensor(-0.0009)   tensor(-7.0173)\n",
              "10      tensor(-0.0002)   tensor(-8.7272)\n",
              "11      tensor(-0.0081)   tensor(-4.8145)\n",
              "12      tensor(-1.6897)   tensor(-0.2040)\n",
              "13      tensor(-0.3346)   tensor(-1.2574)\n",
              "14      tensor(-8.1287)   tensor(-0.0003)\n",
              "15      tensor(-0.0054)   tensor(-5.2233)\n",
              "16      tensor(-0.0010)   tensor(-6.9281)\n",
              "17      tensor(-0.7121)   tensor(-0.6745)\n",
              "18      tensor(-0.0137)   tensor(-4.2996)\n",
              "19      tensor(-5.8647)   tensor(-0.0028)\n",
              "20      tensor(-6.3836)   tensor(-0.0017)\n",
              "21      tensor(-0.0094)   tensor(-4.6732)\n",
              "22      tensor(-0.0003)   tensor(-8.0771)\n",
              "23      tensor(-6.5018)   tensor(-0.0015)\n",
              "24      tensor(-0.0165)   tensor(-4.1130)\n",
              "25      tensor(-0.3060)   tensor(-1.3334)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsqRq3Z9hR38"
      },
      "source": [
        "# 손글씨 이미지 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76bPyEeIhGgS"
      },
      "source": [
        "# 파이토치 라이브러리 임포트\r\n",
        "import torch\r\n",
        "from torch.autograd import Variable  # 자동미분기능 제공... 흠..\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import DataLoader, TensorDataset\r\n",
        "\r\n",
        "# sklearn 라이브러리 임포트\r\n",
        "from sklearn.datasets import load_digits\r\n",
        "from sklearn.datasets import fetch_openml  # 다운싫어\r\n",
        "from sklearn import datasets, model_selection\r\n",
        "\r\n",
        "# pandas 라이브러리 임포트\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# matplotlib 라이브러리 임포트\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from matplotlib import cm\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0az-izU2m_c",
        "outputId": "d5fcfd3a-49f4-480a-c5c1-00e5f4a55f7b"
      },
      "source": [
        "# GPU로 실행, 모델이랑 데이터를 to.(device)로 GPU로 보냄\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrdXGwt4jj-4"
      },
      "source": [
        "* https://teddylee777.github.io/scikit-learn/sklearn%EC%9C%BC%EB%A1%9C-mnist-%EC%86%90%EA%B8%80%EC%94%A8%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJJ30KvuhGdp",
        "outputId": "51e1c109-d793-4c68-ee85-c995d2e0e7f3"
      },
      "source": [
        "# 데이터\r\n",
        "# 다운 받기 싫오\r\n",
        "mnist = fetch_openml('mnist_784')\r\n",
        "mnist.data.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "zQGwSH4KhGbD",
        "outputId": "3f8885b3-94b5-4a4c-d45d-9fb0d291236b"
      },
      "source": [
        "# 정규화\r\n",
        "mnist_data = mnist.data / 255  # 여기서 255는 정규화를 하기 위한 수\r\n",
        "\r\n",
        "pd.DataFrame(mnist_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.576471</td>\n",
              "      <td>0.988235</td>\n",
              "      <td>0.164706</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70000 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2    3    4    5    6    ...  777  778  779  780  781  782  783\n",
              "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
              "69995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "69996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "69997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "69998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "69999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[70000 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JdE6Je1vhGYe",
        "outputId": "d064ae0e-7891-4ee1-f571-7a1841076186"
      },
      "source": [
        "plt.imshow(mnist_data[0].reshape(28,28), cmap= cm.gray_r)\r\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTQRlmzLhGV3",
        "outputId": "2145a95c-8d1e-482d-8699-e1e9fb3ea97d"
      },
      "source": [
        "# 타겟 변수 설정\r\n",
        "mnist_label = mnist.target\r\n",
        "mnist_label"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['5', '0', '4', ..., '4', '5', '6'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_awabG-mtnfX",
        "outputId": "f0814027-e9be-4c94-fc64-0a9f1eb612b0"
      },
      "source": [
        "# 넘파이 타입을 int로 변환\r\n",
        "import numpy as np\r\n",
        "mnist_label = np.array(mnist_label, dtype=int)\r\n",
        "mnist_label"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 4, 5, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn1qAdAehGQt"
      },
      "source": [
        "# 훈련\r\n",
        "train_size = 5000  # 사이즈 결정\r\n",
        "test_size = 500\r\n",
        "\r\n",
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(mnist_data, mnist_label, train_size = train_size, test_size = test_size)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7daqo-FrJCs",
        "outputId": "b644dc67-4ace-4477-ebdb-65c3aafcf11a"
      },
      "source": [
        "train_x.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVxJnIM-rlbU",
        "outputId": "02c2699d-05a0-4b2d-c448-ebfc8aed0555"
      },
      "source": [
        "train_y"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 8, ..., 5, 8, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0x3Pdzgqlrp",
        "outputId": "b94aef7e-4527-4e7d-e5ff-f4e232144eb4"
      },
      "source": [
        "# 넘파이에서 토치 변환시 타입은 object가 되면 안된당!!\r\n",
        "# .to(device)로 GPU로 데이터 전송\r\n",
        "train_x = torch.from_numpy(train_x).float().to(device)\r\n",
        "train_y = torch.from_numpy(train_y).long().to(device)\r\n",
        "\r\n",
        "test_x = torch.from_numpy(test_x).float().to(device)\r\n",
        "test_y = torch.from_numpy(test_y).long().to(device)\r\n",
        "\r\n",
        "print(train_x.shape)\r\n",
        "print(train_y.shape)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5000, 784])\n",
            "torch.Size([5000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UslXj_Gwuxgz",
        "outputId": "664f3952-5f86-4fc2-fbfd-c3725848941f"
      },
      "source": [
        "train = TensorDataset(train_x, train_y)\r\n",
        "\r\n",
        "print(train[0])\r\n",
        "\r\n",
        "# 미니배치로 분할 (100건 단위로 분할)\r\n",
        "train_loader = DataLoader(train, batch_size=100, shuffle=True)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.2510, 0.3490, 0.6431, 0.7843, 0.8196,\n",
            "        0.0824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.1569, 0.2902, 0.5725, 0.9059, 0.9882, 0.9961, 0.9961, 0.9961,\n",
            "        0.9961, 0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
            "        0.4706, 0.8000, 0.9725, 0.9961, 0.9961, 0.9961, 0.7725, 0.3020, 0.1647,\n",
            "        0.5412, 0.9961, 0.5961, 0.4980, 0.3765, 0.0902, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2471, 0.6667,\n",
            "        0.9451, 0.9961, 0.9961, 0.7686, 0.5608, 0.4039, 0.2392, 0.0157, 0.0000,\n",
            "        0.0000, 0.5882, 0.9961, 0.9961, 0.9961, 0.9961, 0.7216, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6706, 0.9922,\n",
            "        0.9961, 0.9333, 0.5608, 0.2314, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0078, 0.3059, 0.9961, 0.9961, 0.9922, 0.4157, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7843, 0.9922,\n",
            "        0.9294, 0.4549, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.2078, 0.8157, 0.9686, 0.9961, 0.7098, 0.2000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "        0.9961, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0314, 0.4824, 0.8118, 0.9686, 0.9961, 0.8431, 0.2980, 0.0353, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.9333, 0.9961, 0.5373, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353,\n",
            "        0.4784, 0.8275, 0.9961, 0.9098, 0.5412, 0.2118, 0.0353, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.2118, 0.9294, 0.9961, 0.9529, 0.6431, 0.3137, 0.1333, 0.0353,\n",
            "        0.6667, 0.9961, 0.9686, 0.5490, 0.1490, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.1098, 0.5922, 0.8824, 0.9961, 0.9961, 0.9569,\n",
            "        0.8627, 0.9961, 0.8588, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0510, 0.1843, 0.2980,\n",
            "        0.9608, 0.9961, 0.9961, 0.8431, 0.1725, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.6627, 0.9961, 0.8196, 0.6000, 0.9765, 0.9647, 0.4745, 0.0275, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.1137, 0.9490, 0.8980, 0.0667, 0.0000, 0.2431, 0.8510, 0.9961, 0.4000,\n",
            "        0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.5098, 0.9961, 0.6314, 0.0000, 0.0000, 0.0000, 0.0392, 0.5961,\n",
            "        0.9961, 0.3647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.5922, 0.9961, 0.4392, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.4784, 0.9961, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.3333, 0.9725, 0.6549, 0.0627, 0.0000, 0.0471,\n",
            "        0.1059, 0.6784, 0.9961, 0.4235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4431, 0.9922, 0.8392, 0.7098,\n",
            "        0.9529, 0.9961, 0.9961, 0.7843, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4627, 0.8235,\n",
            "        0.9765, 0.9765, 0.8118, 0.4824, 0.1647, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000], device='cuda:0'), tensor(8, device='cuda:0'))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1J3PEgYz9TT",
        "outputId": "97863ac3-43ab-4925-f7a4-0f686cafde67"
      },
      "source": [
        "for i,b in train_loader:\r\n",
        "  print(b)\r\n",
        "\r\n",
        "  # 임마가 배치사이즈 100개\r\n",
        "  # 전체 텐서는 7개"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4, 6, 3, 0, 1, 5, 0, 7, 2, 4, 0, 4, 2, 1, 8, 9, 5, 2, 9, 1, 4, 7, 4, 7,\n",
            "        1, 2, 2, 3, 8, 4, 2, 6, 9, 6, 0, 6, 0, 1, 5, 7, 6, 3, 9, 2, 9, 7, 8, 5,\n",
            "        3, 9, 5, 1, 2, 2, 9, 4, 0, 0, 1, 1, 1, 2, 8, 4, 0, 4, 7, 5, 9, 8, 3, 8,\n",
            "        4, 5, 7, 9, 3, 9, 4, 7, 4, 0, 8, 7, 4, 7, 2, 2, 1, 1, 8, 9, 4, 7, 2, 9,\n",
            "        4, 1, 3, 6], device='cuda:0')\n",
            "tensor([1, 7, 9, 7, 4, 3, 0, 4, 4, 9, 3, 3, 4, 9, 2, 3, 4, 7, 4, 9, 5, 8, 2, 1,\n",
            "        1, 6, 3, 6, 7, 8, 9, 9, 3, 5, 3, 8, 7, 9, 7, 5, 6, 0, 6, 1, 5, 0, 9, 7,\n",
            "        8, 9, 6, 7, 4, 1, 6, 7, 1, 1, 2, 5, 2, 9, 4, 4, 9, 2, 8, 6, 5, 6, 7, 9,\n",
            "        9, 3, 8, 2, 3, 5, 1, 7, 3, 4, 3, 7, 3, 6, 6, 0, 2, 5, 1, 6, 7, 1, 1, 9,\n",
            "        9, 5, 9, 4], device='cuda:0')\n",
            "tensor([3, 3, 1, 5, 1, 4, 0, 2, 9, 1, 8, 7, 6, 8, 0, 8, 3, 3, 4, 0, 1, 3, 3, 6,\n",
            "        0, 1, 2, 2, 7, 1, 5, 8, 1, 1, 6, 3, 0, 9, 5, 4, 2, 4, 8, 3, 1, 3, 9, 8,\n",
            "        7, 7, 5, 2, 3, 9, 0, 5, 5, 7, 1, 5, 8, 6, 6, 1, 8, 3, 2, 0, 1, 2, 6, 9,\n",
            "        4, 7, 7, 8, 5, 1, 8, 6, 6, 1, 1, 9, 0, 0, 7, 9, 5, 3, 8, 3, 0, 4, 7, 8,\n",
            "        7, 0, 6, 6], device='cuda:0')\n",
            "tensor([6, 4, 5, 0, 9, 8, 9, 3, 3, 8, 2, 0, 2, 9, 8, 9, 1, 9, 8, 2, 9, 9, 3, 0,\n",
            "        3, 2, 9, 3, 2, 8, 3, 0, 9, 1, 2, 2, 7, 6, 6, 2, 7, 0, 1, 0, 3, 1, 1, 7,\n",
            "        7, 1, 3, 7, 4, 1, 2, 2, 8, 2, 4, 2, 2, 0, 6, 0, 7, 2, 9, 9, 5, 9, 7, 7,\n",
            "        3, 7, 7, 6, 1, 9, 2, 6, 2, 4, 8, 6, 7, 0, 7, 8, 1, 6, 8, 6, 8, 3, 0, 5,\n",
            "        7, 8, 2, 7], device='cuda:0')\n",
            "tensor([6, 8, 5, 9, 7, 2, 8, 2, 3, 3, 9, 7, 3, 3, 4, 1, 4, 5, 9, 1, 1, 8, 9, 8,\n",
            "        6, 6, 3, 2, 1, 8, 0, 1, 5, 5, 9, 3, 8, 7, 3, 1, 1, 7, 5, 2, 7, 0, 7, 1,\n",
            "        1, 9, 6, 7, 0, 1, 2, 4, 5, 3, 4, 7, 7, 1, 6, 8, 8, 4, 4, 0, 9, 0, 0, 7,\n",
            "        8, 4, 2, 2, 4, 1, 3, 1, 2, 3, 3, 4, 3, 9, 1, 6, 5, 6, 0, 5, 2, 0, 6, 6,\n",
            "        9, 0, 4, 9], device='cuda:0')\n",
            "tensor([4, 3, 6, 2, 9, 0, 4, 4, 9, 6, 5, 9, 0, 3, 5, 7, 7, 2, 1, 1, 9, 2, 8, 6,\n",
            "        8, 0, 3, 1, 3, 7, 5, 1, 0, 3, 7, 3, 4, 1, 3, 7, 6, 2, 7, 4, 2, 4, 3, 6,\n",
            "        7, 4, 0, 1, 0, 4, 4, 4, 3, 3, 9, 2, 7, 4, 0, 4, 1, 0, 6, 1, 8, 4, 7, 1,\n",
            "        2, 4, 6, 4, 5, 6, 8, 6, 6, 6, 5, 4, 2, 8, 3, 8, 8, 8, 7, 6, 8, 6, 2, 1,\n",
            "        1, 4, 9, 6], device='cuda:0')\n",
            "tensor([6, 6, 3, 2, 8, 7, 6, 7, 5, 5, 0, 9, 1, 3, 7, 0, 3, 4, 6, 7, 0, 4, 9, 5,\n",
            "        9, 7, 7, 9, 8, 1, 0, 0, 4, 1, 1, 7, 3, 5, 6, 4, 8, 1, 1, 5, 9, 4, 0, 1,\n",
            "        5, 1, 8, 4, 2, 5, 3, 8, 2, 3, 7, 4, 1, 7, 8, 6, 8, 1, 0, 9, 9, 7, 4, 7,\n",
            "        7, 5, 2, 3, 9, 5, 2, 4, 6, 8, 7, 6, 5, 9, 6, 0, 9, 2, 5, 6, 7, 2, 4, 2,\n",
            "        3, 5, 4, 5], device='cuda:0')\n",
            "tensor([4, 1, 9, 7, 9, 3, 4, 6, 0, 1, 5, 3, 2, 9, 1, 6, 0, 6, 3, 3, 2, 9, 0, 6,\n",
            "        5, 0, 1, 3, 5, 4, 1, 7, 6, 3, 7, 9, 7, 4, 2, 8, 6, 4, 0, 6, 8, 1, 5, 3,\n",
            "        8, 7, 0, 9, 3, 5, 3, 8, 2, 0, 9, 9, 9, 4, 9, 9, 4, 0, 1, 5, 9, 3, 3, 1,\n",
            "        3, 5, 1, 0, 5, 6, 9, 4, 2, 4, 7, 0, 2, 6, 2, 5, 4, 0, 5, 8, 1, 0, 8, 9,\n",
            "        3, 4, 5, 8], device='cuda:0')\n",
            "tensor([1, 3, 2, 4, 3, 1, 0, 7, 9, 4, 0, 7, 7, 0, 2, 1, 1, 5, 5, 8, 2, 1, 4, 4,\n",
            "        9, 4, 5, 3, 5, 4, 3, 4, 7, 8, 3, 3, 6, 6, 7, 5, 9, 0, 3, 3, 7, 4, 3, 2,\n",
            "        8, 1, 1, 2, 6, 8, 7, 9, 6, 0, 6, 7, 8, 9, 4, 0, 5, 5, 9, 3, 0, 7, 9, 6,\n",
            "        7, 8, 1, 3, 4, 5, 0, 2, 8, 5, 3, 1, 1, 2, 8, 3, 3, 0, 2, 4, 1, 0, 2, 2,\n",
            "        1, 6, 7, 8], device='cuda:0')\n",
            "tensor([8, 2, 9, 3, 8, 0, 8, 2, 4, 9, 8, 9, 8, 2, 1, 0, 6, 3, 0, 2, 6, 5, 6, 4,\n",
            "        2, 4, 1, 6, 5, 6, 2, 7, 1, 4, 1, 6, 7, 8, 3, 9, 2, 7, 0, 2, 2, 6, 2, 1,\n",
            "        0, 8, 2, 0, 8, 2, 6, 8, 5, 2, 1, 0, 1, 4, 4, 0, 4, 0, 2, 7, 9, 3, 3, 2,\n",
            "        9, 2, 6, 6, 3, 7, 9, 0, 0, 2, 1, 0, 3, 1, 1, 7, 5, 5, 8, 8, 5, 5, 1, 8,\n",
            "        8, 1, 6, 3], device='cuda:0')\n",
            "tensor([3, 7, 8, 7, 7, 2, 4, 2, 0, 1, 1, 5, 3, 5, 4, 7, 7, 1, 1, 4, 3, 4, 1, 3,\n",
            "        7, 1, 7, 3, 7, 8, 8, 3, 5, 2, 1, 3, 1, 5, 4, 5, 1, 6, 3, 6, 1, 8, 2, 7,\n",
            "        6, 1, 8, 9, 0, 0, 9, 1, 2, 9, 8, 1, 9, 6, 8, 9, 3, 7, 1, 4, 7, 0, 3, 2,\n",
            "        3, 3, 9, 4, 5, 7, 1, 5, 6, 0, 1, 4, 3, 9, 4, 5, 2, 6, 6, 4, 7, 0, 2, 6,\n",
            "        3, 3, 6, 7], device='cuda:0')\n",
            "tensor([1, 5, 3, 0, 2, 5, 6, 3, 7, 2, 0, 9, 4, 3, 2, 5, 7, 8, 8, 1, 4, 2, 0, 3,\n",
            "        4, 0, 5, 9, 8, 4, 5, 8, 8, 9, 7, 4, 0, 9, 4, 3, 6, 6, 8, 9, 1, 0, 2, 2,\n",
            "        4, 7, 8, 5, 0, 7, 2, 1, 7, 7, 4, 3, 5, 4, 0, 9, 5, 4, 2, 5, 2, 2, 1, 1,\n",
            "        1, 9, 6, 9, 8, 9, 1, 2, 6, 1, 4, 5, 6, 9, 8, 0, 9, 6, 8, 6, 1, 1, 6, 6,\n",
            "        6, 8, 2, 0], device='cuda:0')\n",
            "tensor([4, 4, 2, 8, 2, 2, 7, 2, 8, 6, 0, 5, 2, 4, 5, 8, 4, 8, 7, 3, 0, 5, 1, 9,\n",
            "        5, 4, 9, 8, 7, 9, 5, 0, 8, 2, 1, 6, 6, 2, 4, 8, 5, 0, 6, 4, 7, 3, 6, 3,\n",
            "        1, 8, 6, 8, 5, 6, 9, 3, 4, 4, 9, 8, 8, 8, 5, 4, 4, 8, 6, 0, 4, 4, 6, 0,\n",
            "        9, 3, 4, 7, 7, 5, 9, 4, 2, 3, 9, 4, 1, 4, 0, 7, 4, 6, 4, 1, 6, 2, 8, 4,\n",
            "        1, 5, 1, 4], device='cuda:0')\n",
            "tensor([6, 3, 7, 4, 3, 2, 1, 0, 1, 9, 0, 7, 0, 7, 1, 4, 7, 2, 7, 0, 1, 1, 4, 0,\n",
            "        4, 3, 0, 4, 3, 4, 8, 3, 9, 2, 3, 6, 7, 0, 7, 3, 7, 0, 6, 5, 4, 4, 3, 8,\n",
            "        0, 1, 9, 6, 2, 7, 3, 9, 1, 2, 3, 4, 3, 2, 9, 6, 6, 3, 0, 7, 5, 0, 8, 8,\n",
            "        7, 4, 5, 1, 7, 5, 0, 9, 3, 7, 3, 1, 4, 8, 7, 5, 6, 3, 3, 2, 7, 2, 6, 4,\n",
            "        7, 8, 2, 1], device='cuda:0')\n",
            "tensor([0, 3, 8, 6, 7, 1, 2, 1, 3, 3, 8, 6, 4, 6, 7, 2, 2, 7, 6, 4, 3, 8, 3, 5,\n",
            "        2, 7, 7, 2, 9, 5, 4, 1, 9, 4, 0, 9, 3, 9, 3, 3, 8, 1, 5, 6, 2, 0, 9, 5,\n",
            "        1, 1, 4, 6, 8, 5, 7, 2, 4, 1, 2, 1, 1, 0, 1, 8, 5, 7, 1, 1, 9, 1, 8, 6,\n",
            "        3, 2, 7, 1, 6, 5, 9, 1, 6, 5, 6, 8, 2, 4, 2, 1, 3, 3, 7, 5, 7, 1, 0, 6,\n",
            "        4, 9, 3, 6], device='cuda:0')\n",
            "tensor([5, 3, 3, 2, 3, 8, 5, 2, 0, 3, 4, 5, 5, 1, 8, 4, 2, 0, 9, 8, 9, 5, 1, 9,\n",
            "        4, 5, 0, 7, 2, 8, 4, 9, 6, 1, 1, 4, 1, 1, 4, 3, 8, 3, 5, 3, 5, 7, 3, 6,\n",
            "        1, 1, 9, 8, 9, 3, 7, 5, 6, 6, 3, 9, 3, 6, 6, 0, 5, 7, 2, 9, 7, 6, 1, 0,\n",
            "        6, 3, 7, 0, 3, 8, 5, 0, 1, 7, 0, 7, 4, 7, 9, 7, 8, 7, 4, 7, 6, 5, 6, 7,\n",
            "        9, 8, 3, 9], device='cuda:0')\n",
            "tensor([4, 5, 0, 9, 3, 5, 5, 9, 4, 1, 9, 5, 1, 1, 2, 8, 0, 0, 6, 7, 0, 3, 4, 7,\n",
            "        6, 8, 7, 1, 5, 7, 3, 7, 2, 0, 9, 1, 3, 4, 6, 2, 6, 6, 4, 2, 7, 8, 7, 7,\n",
            "        4, 5, 4, 1, 2, 3, 6, 2, 5, 0, 4, 1, 2, 4, 5, 2, 3, 6, 6, 1, 2, 8, 8, 0,\n",
            "        2, 9, 8, 6, 4, 4, 9, 8, 5, 7, 2, 5, 4, 5, 2, 4, 7, 5, 4, 4, 6, 1, 1, 2,\n",
            "        0, 6, 7, 4], device='cuda:0')\n",
            "tensor([7, 8, 9, 9, 7, 0, 0, 6, 9, 4, 4, 2, 6, 0, 6, 5, 1, 4, 5, 3, 2, 7, 4, 7,\n",
            "        3, 8, 1, 2, 5, 5, 1, 4, 0, 9, 4, 6, 3, 2, 3, 5, 7, 8, 5, 9, 7, 8, 9, 3,\n",
            "        5, 5, 8, 2, 7, 4, 2, 8, 3, 8, 7, 7, 1, 1, 6, 7, 5, 1, 7, 5, 3, 0, 6, 3,\n",
            "        2, 5, 0, 5, 2, 7, 4, 3, 8, 2, 7, 3, 9, 7, 8, 5, 6, 3, 3, 9, 9, 9, 7, 8,\n",
            "        6, 0, 8, 8], device='cuda:0')\n",
            "tensor([9, 1, 2, 3, 6, 6, 7, 6, 4, 8, 8, 4, 1, 3, 8, 3, 2, 1, 3, 1, 3, 2, 5, 6,\n",
            "        3, 5, 6, 0, 4, 6, 3, 8, 0, 6, 0, 4, 0, 9, 3, 8, 3, 8, 8, 4, 8, 8, 5, 5,\n",
            "        8, 0, 1, 0, 3, 4, 7, 4, 4, 6, 0, 6, 2, 0, 1, 4, 1, 9, 6, 6, 4, 9, 1, 9,\n",
            "        7, 2, 1, 6, 0, 2, 7, 3, 0, 1, 2, 1, 2, 5, 1, 8, 6, 3, 7, 0, 6, 5, 1, 7,\n",
            "        1, 2, 1, 8], device='cuda:0')\n",
            "tensor([2, 9, 7, 0, 2, 8, 6, 0, 9, 5, 6, 4, 3, 1, 9, 6, 7, 3, 1, 2, 9, 7, 1, 0,\n",
            "        2, 9, 2, 2, 2, 1, 1, 3, 8, 8, 5, 9, 6, 8, 7, 4, 2, 1, 6, 0, 1, 7, 9, 1,\n",
            "        7, 7, 2, 6, 0, 3, 9, 0, 4, 6, 2, 2, 2, 0, 9, 9, 4, 6, 3, 5, 5, 9, 0, 6,\n",
            "        9, 8, 8, 1, 0, 0, 9, 1, 6, 5, 1, 1, 8, 5, 7, 7, 4, 9, 8, 2, 8, 7, 4, 3,\n",
            "        2, 2, 2, 4], device='cuda:0')\n",
            "tensor([2, 2, 1, 1, 6, 0, 5, 2, 0, 9, 9, 0, 1, 9, 6, 2, 7, 9, 7, 5, 9, 5, 0, 7,\n",
            "        7, 5, 3, 0, 6, 9, 6, 8, 6, 8, 8, 4, 8, 0, 1, 0, 7, 7, 4, 5, 6, 5, 7, 1,\n",
            "        7, 5, 4, 7, 8, 9, 9, 9, 5, 1, 8, 8, 8, 8, 7, 8, 0, 5, 1, 2, 0, 2, 9, 0,\n",
            "        6, 2, 1, 3, 0, 9, 1, 4, 0, 2, 4, 6, 9, 0, 3, 4, 6, 5, 3, 6, 5, 0, 9, 8,\n",
            "        6, 8, 1, 1], device='cuda:0')\n",
            "tensor([9, 7, 2, 2, 4, 7, 8, 4, 5, 9, 8, 3, 0, 2, 7, 4, 7, 5, 0, 3, 2, 4, 2, 2,\n",
            "        8, 7, 7, 3, 2, 5, 0, 7, 1, 1, 8, 6, 1, 7, 4, 4, 0, 3, 3, 7, 0, 9, 8, 4,\n",
            "        9, 7, 4, 0, 7, 6, 7, 4, 1, 1, 5, 7, 1, 2, 8, 8, 4, 7, 5, 4, 5, 6, 0, 4,\n",
            "        0, 8, 1, 6, 7, 5, 3, 6, 8, 9, 0, 3, 8, 0, 5, 5, 0, 8, 3, 3, 7, 9, 6, 0,\n",
            "        2, 4, 3, 8], device='cuda:0')\n",
            "tensor([9, 2, 4, 4, 2, 1, 8, 3, 7, 3, 1, 0, 7, 6, 2, 7, 6, 5, 1, 8, 3, 3, 9, 7,\n",
            "        3, 3, 6, 7, 5, 6, 7, 7, 5, 6, 9, 0, 1, 7, 6, 2, 9, 6, 1, 5, 7, 7, 0, 1,\n",
            "        2, 6, 9, 4, 5, 8, 7, 1, 9, 7, 7, 5, 6, 1, 2, 1, 8, 1, 9, 2, 0, 9, 5, 1,\n",
            "        0, 9, 8, 8, 6, 9, 0, 2, 6, 7, 0, 8, 7, 8, 2, 6, 9, 4, 1, 3, 7, 6, 9, 7,\n",
            "        0, 9, 6, 8], device='cuda:0')\n",
            "tensor([8, 1, 3, 9, 5, 2, 4, 4, 3, 1, 8, 3, 9, 9, 6, 2, 3, 4, 8, 3, 6, 5, 3, 0,\n",
            "        7, 4, 8, 5, 8, 4, 5, 8, 0, 1, 3, 6, 2, 6, 2, 8, 0, 5, 9, 7, 1, 7, 8, 1,\n",
            "        5, 4, 1, 5, 4, 1, 8, 1, 2, 1, 3, 4, 9, 7, 6, 7, 2, 9, 9, 8, 4, 2, 3, 1,\n",
            "        5, 7, 6, 1, 2, 3, 6, 4, 0, 3, 3, 3, 3, 1, 9, 6, 8, 5, 2, 6, 2, 8, 9, 5,\n",
            "        3, 8, 4, 3], device='cuda:0')\n",
            "tensor([4, 9, 3, 9, 2, 2, 1, 8, 8, 1, 7, 1, 7, 0, 4, 6, 6, 1, 6, 0, 6, 1, 3, 3,\n",
            "        9, 6, 2, 4, 2, 1, 5, 6, 0, 7, 0, 9, 8, 5, 3, 4, 2, 4, 7, 5, 8, 0, 6, 6,\n",
            "        2, 0, 5, 9, 9, 9, 4, 5, 2, 8, 6, 7, 5, 0, 1, 7, 8, 4, 0, 2, 3, 0, 0, 7,\n",
            "        9, 7, 1, 0, 8, 8, 3, 4, 5, 8, 2, 9, 6, 7, 0, 1, 0, 6, 2, 5, 1, 6, 4, 7,\n",
            "        7, 3, 5, 7], device='cuda:0')\n",
            "tensor([2, 4, 9, 7, 5, 3, 2, 4, 1, 9, 1, 8, 6, 6, 5, 2, 3, 3, 1, 4, 0, 3, 9, 9,\n",
            "        3, 1, 5, 1, 3, 4, 5, 6, 1, 6, 8, 7, 4, 9, 9, 9, 4, 2, 9, 4, 2, 9, 3, 5,\n",
            "        9, 4, 6, 7, 9, 3, 8, 0, 8, 9, 0, 8, 9, 2, 4, 3, 0, 9, 6, 9, 7, 4, 8, 5,\n",
            "        8, 7, 1, 1, 4, 3, 9, 7, 1, 9, 6, 3, 2, 4, 2, 8, 7, 7, 7, 4, 0, 6, 1, 7,\n",
            "        9, 4, 5, 4], device='cuda:0')\n",
            "tensor([0, 7, 7, 8, 5, 6, 8, 5, 2, 1, 8, 7, 5, 8, 2, 2, 3, 6, 2, 0, 6, 8, 3, 3,\n",
            "        1, 4, 6, 6, 1, 8, 8, 5, 7, 1, 8, 5, 0, 4, 0, 5, 4, 1, 2, 0, 3, 8, 1, 4,\n",
            "        9, 9, 4, 6, 5, 7, 8, 0, 2, 3, 4, 1, 3, 6, 6, 1, 9, 0, 3, 6, 4, 7, 6, 1,\n",
            "        7, 0, 6, 4, 3, 7, 0, 1, 6, 4, 9, 4, 1, 5, 0, 3, 0, 5, 7, 6, 5, 4, 1, 7,\n",
            "        2, 4, 4, 6], device='cuda:0')\n",
            "tensor([8, 5, 2, 6, 3, 1, 2, 9, 1, 7, 8, 4, 5, 9, 8, 4, 7, 4, 1, 2, 6, 3, 9, 7,\n",
            "        3, 9, 2, 5, 4, 7, 3, 6, 3, 3, 0, 9, 8, 2, 5, 8, 4, 4, 1, 1, 5, 8, 5, 4,\n",
            "        1, 2, 5, 3, 0, 6, 3, 9, 9, 3, 4, 2, 4, 0, 1, 0, 8, 0, 5, 4, 4, 3, 1, 7,\n",
            "        2, 7, 4, 6, 9, 1, 9, 6, 5, 7, 1, 8, 6, 7, 3, 9, 7, 0, 5, 1, 1, 9, 0, 9,\n",
            "        3, 2, 3, 8], device='cuda:0')\n",
            "tensor([7, 1, 2, 0, 2, 0, 9, 6, 5, 1, 7, 2, 9, 9, 4, 3, 4, 6, 6, 5, 3, 4, 9, 4,\n",
            "        6, 8, 7, 5, 4, 9, 8, 6, 1, 7, 8, 3, 8, 8, 0, 4, 4, 8, 9, 1, 6, 5, 3, 9,\n",
            "        6, 7, 0, 5, 8, 4, 4, 7, 9, 9, 9, 7, 4, 0, 6, 4, 4, 1, 8, 7, 8, 0, 0, 0,\n",
            "        1, 7, 0, 6, 0, 9, 2, 3, 3, 1, 0, 7, 8, 2, 5, 1, 3, 6, 0, 6, 1, 2, 9, 1,\n",
            "        8, 0, 6, 9], device='cuda:0')\n",
            "tensor([4, 0, 1, 4, 0, 3, 2, 3, 0, 0, 6, 2, 3, 6, 4, 8, 0, 5, 5, 2, 4, 1, 0, 3,\n",
            "        9, 8, 0, 6, 9, 3, 3, 8, 6, 6, 0, 3, 1, 6, 7, 4, 6, 0, 5, 5, 9, 3, 3, 0,\n",
            "        1, 9, 1, 6, 2, 4, 5, 6, 1, 1, 0, 4, 1, 6, 4, 1, 2, 1, 3, 2, 0, 3, 8, 1,\n",
            "        6, 8, 0, 5, 4, 8, 3, 7, 1, 9, 1, 5, 1, 8, 3, 8, 7, 7, 1, 1, 9, 7, 4, 7,\n",
            "        9, 4, 6, 1], device='cuda:0')\n",
            "tensor([8, 1, 6, 6, 3, 2, 7, 2, 6, 4, 8, 1, 0, 9, 7, 8, 4, 3, 1, 5, 8, 5, 6, 6,\n",
            "        9, 7, 2, 0, 2, 3, 1, 3, 7, 1, 6, 4, 0, 5, 8, 8, 6, 1, 7, 8, 3, 5, 1, 9,\n",
            "        1, 5, 8, 8, 5, 0, 9, 2, 9, 4, 9, 2, 1, 3, 9, 7, 1, 0, 5, 0, 7, 6, 4, 1,\n",
            "        3, 9, 9, 1, 0, 7, 5, 3, 8, 4, 9, 5, 0, 4, 1, 4, 3, 5, 8, 0, 6, 8, 8, 2,\n",
            "        3, 0, 0, 8], device='cuda:0')\n",
            "tensor([1, 0, 2, 9, 9, 3, 2, 2, 0, 8, 5, 9, 2, 4, 1, 9, 8, 6, 9, 2, 0, 8, 8, 0,\n",
            "        6, 6, 6, 1, 9, 1, 5, 9, 4, 9, 0, 0, 6, 2, 6, 1, 3, 9, 1, 4, 1, 9, 3, 3,\n",
            "        4, 7, 2, 5, 0, 6, 7, 6, 5, 4, 4, 8, 6, 7, 2, 6, 3, 3, 3, 7, 6, 3, 7, 3,\n",
            "        4, 1, 4, 5, 8, 6, 6, 9, 6, 2, 6, 5, 6, 0, 0, 1, 9, 9, 6, 2, 5, 2, 2, 5,\n",
            "        9, 1, 5, 4], device='cuda:0')\n",
            "tensor([2, 0, 7, 3, 1, 6, 8, 1, 9, 5, 9, 6, 7, 2, 0, 8, 7, 9, 4, 3, 8, 7, 7, 3,\n",
            "        5, 5, 7, 4, 8, 4, 5, 7, 8, 7, 3, 1, 3, 7, 5, 2, 3, 2, 3, 4, 6, 3, 0, 2,\n",
            "        7, 1, 5, 6, 8, 5, 5, 2, 3, 6, 6, 4, 9, 0, 5, 8, 4, 8, 1, 3, 5, 6, 5, 1,\n",
            "        3, 5, 3, 3, 9, 9, 4, 6, 9, 4, 6, 0, 0, 1, 8, 4, 7, 6, 6, 1, 6, 1, 5, 4,\n",
            "        1, 1, 7, 2], device='cuda:0')\n",
            "tensor([9, 1, 1, 0, 4, 7, 4, 9, 5, 6, 7, 7, 2, 2, 9, 1, 3, 5, 3, 1, 8, 6, 4, 6,\n",
            "        1, 5, 0, 0, 9, 0, 9, 0, 2, 7, 4, 3, 2, 9, 7, 7, 0, 7, 7, 0, 6, 9, 3, 7,\n",
            "        9, 9, 7, 4, 3, 0, 0, 0, 9, 6, 4, 4, 5, 7, 9, 0, 6, 0, 0, 2, 8, 3, 3, 1,\n",
            "        3, 0, 9, 8, 4, 7, 7, 1, 1, 1, 6, 6, 7, 0, 5, 3, 9, 4, 1, 4, 9, 5, 1, 9,\n",
            "        2, 8, 7, 8], device='cuda:0')\n",
            "tensor([8, 3, 1, 7, 7, 6, 0, 5, 0, 8, 6, 6, 4, 6, 7, 5, 2, 7, 2, 8, 2, 7, 6, 5,\n",
            "        7, 5, 5, 9, 6, 0, 4, 4, 0, 8, 1, 0, 1, 6, 5, 6, 4, 1, 1, 1, 1, 7, 6, 5,\n",
            "        6, 0, 4, 1, 8, 2, 2, 7, 7, 7, 0, 0, 5, 8, 1, 3, 4, 6, 8, 9, 4, 3, 8, 6,\n",
            "        1, 0, 2, 5, 2, 7, 4, 8, 7, 4, 9, 6, 2, 6, 6, 8, 8, 3, 5, 0, 5, 9, 3, 6,\n",
            "        6, 2, 8, 6], device='cuda:0')\n",
            "tensor([2, 1, 8, 0, 6, 5, 4, 3, 6, 6, 1, 1, 6, 3, 8, 0, 1, 1, 6, 5, 9, 9, 8, 1,\n",
            "        6, 2, 0, 4, 1, 5, 0, 1, 1, 6, 3, 8, 1, 9, 8, 9, 8, 1, 7, 6, 1, 6, 6, 4,\n",
            "        2, 4, 3, 2, 7, 0, 5, 4, 0, 6, 4, 7, 9, 5, 5, 0, 3, 7, 4, 3, 8, 1, 4, 3,\n",
            "        2, 2, 5, 7, 4, 7, 1, 4, 5, 8, 2, 8, 3, 2, 7, 7, 2, 9, 7, 3, 1, 1, 8, 1,\n",
            "        0, 7, 7, 9], device='cuda:0')\n",
            "tensor([0, 2, 0, 9, 4, 8, 6, 3, 2, 9, 7, 7, 8, 7, 0, 3, 3, 4, 2, 3, 3, 5, 7, 1,\n",
            "        5, 8, 3, 5, 5, 4, 9, 7, 5, 7, 1, 2, 6, 2, 8, 9, 3, 9, 2, 4, 9, 8, 2, 9,\n",
            "        5, 3, 5, 8, 9, 7, 9, 4, 0, 6, 6, 5, 5, 4, 7, 5, 2, 2, 0, 8, 7, 1, 3, 5,\n",
            "        4, 8, 1, 3, 5, 1, 8, 3, 2, 0, 9, 9, 9, 5, 2, 4, 1, 5, 4, 6, 6, 6, 8, 9,\n",
            "        4, 1, 0, 3], device='cuda:0')\n",
            "tensor([8, 3, 9, 9, 1, 8, 5, 5, 4, 8, 9, 9, 6, 2, 0, 8, 5, 9, 7, 1, 3, 2, 1, 1,\n",
            "        2, 6, 7, 9, 0, 2, 9, 7, 1, 8, 3, 4, 8, 7, 0, 5, 8, 2, 1, 0, 8, 3, 3, 5,\n",
            "        3, 5, 1, 6, 7, 7, 4, 9, 6, 3, 2, 5, 3, 9, 7, 7, 2, 0, 6, 3, 7, 1, 8, 9,\n",
            "        7, 6, 4, 6, 7, 1, 1, 3, 1, 8, 3, 8, 5, 3, 9, 4, 0, 6, 7, 3, 9, 8, 7, 5,\n",
            "        3, 4, 6, 3], device='cuda:0')\n",
            "tensor([4, 3, 1, 5, 0, 3, 1, 5, 9, 3, 6, 5, 0, 3, 8, 9, 0, 1, 1, 4, 6, 0, 7, 7,\n",
            "        7, 0, 0, 4, 7, 0, 9, 5, 1, 6, 0, 9, 2, 0, 9, 7, 4, 6, 9, 2, 9, 4, 5, 7,\n",
            "        6, 9, 7, 0, 4, 5, 1, 2, 2, 2, 2, 8, 6, 1, 9, 6, 2, 4, 8, 5, 1, 7, 6, 9,\n",
            "        6, 0, 0, 4, 2, 9, 8, 6, 6, 0, 8, 5, 1, 0, 3, 2, 3, 0, 7, 7, 1, 0, 7, 9,\n",
            "        8, 7, 2, 3], device='cuda:0')\n",
            "tensor([2, 8, 5, 1, 4, 3, 0, 8, 7, 5, 5, 2, 8, 7, 4, 5, 3, 7, 4, 9, 0, 0, 9, 3,\n",
            "        6, 8, 3, 1, 8, 9, 3, 7, 1, 0, 0, 9, 1, 9, 9, 0, 5, 0, 7, 4, 4, 3, 4, 2,\n",
            "        3, 5, 5, 5, 1, 0, 2, 1, 6, 0, 7, 7, 5, 2, 3, 4, 4, 9, 5, 3, 2, 0, 8, 1,\n",
            "        7, 0, 8, 9, 8, 1, 1, 2, 1, 4, 4, 1, 9, 5, 2, 1, 5, 4, 8, 2, 7, 2, 3, 2,\n",
            "        2, 9, 2, 2], device='cuda:0')\n",
            "tensor([4, 1, 0, 8, 4, 3, 3, 0, 1, 9, 9, 8, 8, 5, 1, 4, 9, 3, 1, 2, 4, 1, 8, 9,\n",
            "        1, 1, 0, 4, 1, 6, 5, 7, 6, 6, 6, 3, 0, 8, 1, 2, 2, 7, 9, 9, 6, 4, 5, 0,\n",
            "        3, 3, 6, 7, 4, 2, 8, 9, 4, 3, 1, 0, 7, 8, 0, 4, 4, 0, 8, 7, 7, 2, 8, 4,\n",
            "        9, 3, 3, 4, 3, 6, 9, 7, 9, 8, 1, 3, 8, 2, 9, 8, 8, 7, 8, 3, 1, 8, 2, 7,\n",
            "        8, 3, 2, 5], device='cuda:0')\n",
            "tensor([9, 5, 1, 8, 9, 0, 7, 9, 0, 7, 3, 3, 0, 6, 3, 7, 1, 2, 4, 7, 5, 9, 6, 2,\n",
            "        1, 5, 9, 2, 2, 6, 6, 9, 1, 2, 7, 6, 1, 2, 6, 0, 6, 9, 4, 7, 6, 4, 1, 9,\n",
            "        4, 7, 5, 5, 6, 2, 2, 7, 0, 0, 4, 1, 2, 5, 7, 6, 6, 1, 3, 5, 8, 1, 3, 9,\n",
            "        9, 8, 1, 0, 8, 1, 4, 5, 8, 5, 0, 7, 9, 7, 8, 5, 9, 4, 6, 4, 2, 8, 7, 1,\n",
            "        8, 0, 5, 4], device='cuda:0')\n",
            "tensor([4, 0, 2, 1, 1, 4, 5, 8, 4, 8, 6, 5, 6, 2, 3, 3, 0, 1, 7, 8, 1, 9, 1, 5,\n",
            "        5, 5, 2, 8, 8, 7, 2, 5, 5, 0, 0, 4, 3, 6, 9, 4, 5, 0, 9, 4, 9, 0, 0, 1,\n",
            "        1, 7, 7, 2, 0, 2, 1, 1, 4, 2, 8, 0, 3, 2, 8, 9, 1, 4, 7, 5, 9, 0, 7, 6,\n",
            "        9, 6, 3, 3, 7, 2, 6, 3, 2, 2, 8, 1, 2, 9, 6, 6, 7, 6, 8, 6, 1, 9, 6, 7,\n",
            "        2, 6, 8, 1], device='cuda:0')\n",
            "tensor([3, 5, 2, 1, 9, 8, 0, 6, 4, 5, 4, 7, 9, 5, 1, 8, 1, 4, 9, 4, 3, 9, 4, 8,\n",
            "        4, 1, 7, 4, 6, 7, 0, 8, 9, 9, 9, 7, 3, 4, 0, 4, 9, 7, 5, 7, 2, 2, 6, 7,\n",
            "        8, 2, 0, 2, 6, 0, 3, 3, 6, 2, 7, 2, 4, 0, 2, 1, 3, 0, 5, 5, 1, 0, 2, 6,\n",
            "        7, 1, 6, 7, 7, 1, 1, 8, 8, 6, 3, 1, 9, 9, 9, 6, 2, 0, 5, 1, 5, 7, 8, 2,\n",
            "        9, 1, 1, 7], device='cuda:0')\n",
            "tensor([0, 7, 8, 8, 9, 3, 4, 8, 5, 9, 0, 9, 3, 7, 5, 0, 6, 0, 0, 6, 4, 7, 0, 1,\n",
            "        2, 2, 1, 3, 6, 4, 1, 7, 4, 6, 1, 9, 8, 7, 7, 4, 0, 8, 1, 4, 1, 1, 3, 8,\n",
            "        3, 3, 3, 6, 7, 6, 8, 9, 8, 4, 2, 6, 8, 7, 8, 6, 4, 5, 5, 9, 6, 6, 1, 7,\n",
            "        9, 0, 7, 9, 1, 5, 9, 9, 7, 5, 2, 3, 8, 3, 0, 1, 0, 1, 5, 8, 0, 4, 9, 2,\n",
            "        6, 7, 9, 5], device='cuda:0')\n",
            "tensor([4, 6, 6, 4, 2, 2, 6, 6, 1, 5, 6, 1, 0, 4, 1, 9, 4, 9, 8, 4, 7, 1, 8, 4,\n",
            "        3, 0, 5, 0, 6, 6, 6, 9, 3, 5, 8, 0, 5, 3, 7, 1, 3, 4, 2, 0, 6, 2, 4, 3,\n",
            "        6, 4, 2, 2, 9, 6, 7, 4, 0, 6, 8, 4, 9, 1, 5, 3, 3, 7, 0, 1, 9, 1, 9, 3,\n",
            "        4, 9, 2, 7, 3, 1, 7, 4, 1, 9, 9, 6, 9, 7, 3, 9, 6, 0, 0, 5, 6, 7, 3, 7,\n",
            "        2, 1, 6, 6], device='cuda:0')\n",
            "tensor([2, 8, 9, 7, 0, 5, 5, 1, 7, 4, 9, 0, 7, 4, 6, 0, 5, 0, 0, 8, 1, 2, 1, 9,\n",
            "        3, 7, 8, 1, 3, 6, 4, 8, 1, 4, 6, 9, 7, 9, 8, 4, 7, 9, 3, 8, 4, 9, 9, 9,\n",
            "        6, 5, 6, 0, 0, 5, 9, 7, 8, 3, 7, 8, 6, 7, 9, 8, 4, 3, 0, 3, 1, 7, 5, 0,\n",
            "        0, 2, 5, 3, 7, 7, 7, 3, 0, 6, 1, 5, 3, 8, 8, 8, 7, 4, 3, 9, 7, 1, 2, 3,\n",
            "        7, 5, 4, 5], device='cuda:0')\n",
            "tensor([3, 4, 6, 4, 2, 3, 2, 8, 4, 8, 1, 5, 5, 2, 4, 2, 8, 2, 8, 0, 1, 0, 7, 4,\n",
            "        6, 3, 2, 6, 9, 7, 7, 1, 2, 5, 6, 4, 1, 7, 2, 6, 7, 4, 3, 4, 2, 1, 6, 7,\n",
            "        5, 8, 3, 4, 9, 6, 7, 7, 1, 8, 3, 8, 0, 7, 1, 5, 0, 5, 0, 2, 1, 0, 9, 5,\n",
            "        7, 8, 9, 7, 5, 7, 2, 6, 3, 5, 3, 1, 2, 9, 3, 1, 0, 1, 3, 3, 0, 6, 6, 8,\n",
            "        3, 3, 7, 7], device='cuda:0')\n",
            "tensor([0, 0, 9, 9, 5, 7, 7, 1, 5, 4, 2, 8, 9, 5, 9, 9, 3, 9, 2, 8, 7, 9, 5, 1,\n",
            "        8, 3, 5, 9, 6, 1, 3, 2, 3, 7, 3, 3, 6, 9, 6, 6, 0, 6, 7, 1, 5, 4, 2, 0,\n",
            "        2, 0, 5, 9, 0, 6, 5, 2, 9, 6, 3, 4, 6, 3, 2, 5, 6, 6, 1, 6, 4, 2, 8, 2,\n",
            "        0, 1, 2, 1, 9, 1, 4, 6, 1, 4, 0, 3, 5, 3, 1, 6, 2, 5, 0, 7, 7, 5, 9, 7,\n",
            "        3, 2, 0, 7], device='cuda:0')\n",
            "tensor([5, 9, 3, 0, 7, 8, 7, 6, 2, 6, 6, 0, 3, 8, 2, 4, 1, 3, 3, 5, 8, 5, 5, 4,\n",
            "        3, 9, 1, 6, 2, 0, 0, 7, 9, 3, 4, 4, 1, 7, 7, 3, 0, 2, 7, 4, 0, 6, 5, 0,\n",
            "        1, 1, 6, 2, 8, 1, 7, 2, 1, 5, 7, 7, 8, 9, 3, 2, 3, 8, 1, 8, 6, 2, 8, 9,\n",
            "        9, 3, 2, 4, 6, 6, 6, 1, 7, 1, 6, 1, 9, 4, 0, 4, 0, 2, 5, 4, 3, 0, 7, 1,\n",
            "        3, 2, 5, 9], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcAi4cMkuxhA"
      },
      "source": [
        "# 신경망 구성\r\n",
        "class Net(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(Net,self).__init__()\r\n",
        "    self.fc1 = nn.Linear(784,256)\r\n",
        "    self.fc2 = nn.Linear(256,256)\r\n",
        "    self.fc3 = nn.Linear(256,256)\r\n",
        "    self.fc4 = nn.Linear(256,128)\r\n",
        "    self.fc5 = nn.Linear(128,128)\r\n",
        "    self.fc6 = nn.Linear(128,10)\r\n",
        "\r\n",
        "  def forward(self,x):\r\n",
        "    x = F.relu(self.fc1(x))\r\n",
        "    x = F.relu(self.fc2(x))\r\n",
        "    x = F.relu(self.fc3(x))\r\n",
        "    x = F.relu(self.fc4(x))\r\n",
        "    x = F.relu(self.fc5(x))\r\n",
        "    x = F.dropout(x, training=self.training)  # 무엇을 훈련 할지 명시 해야함 (training=self.training)\r\n",
        "    x = self.fc6(x)\r\n",
        "    return F.log_softmax(x)\r\n",
        "\r\n",
        "# 인스턴스 생성\r\n",
        "model = Net().to(device)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceKrjP4RwFyv"
      },
      "source": [
        "> 드롭아웃\r\n",
        "* F.dropout(입력 , p = 각 노드의 드롭아웃 확률)\r\n",
        "* torch.nn.functional.dropout에서는 p값과 무엇을 훈련할지 명시 해야함 (training=self.training)!!!\r\n",
        "\r\n",
        "> Variable\r\n",
        "* Variable()\r\n",
        "* 미분을 가능하게 : \".backward()\"에 사용하는 변수\r\n",
        "* https://dororongju.tistory.com/142"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsftXnobyONR",
        "outputId": "ab033fe4-eeb7-4a0c-cd87-4116bd8ca779"
      },
      "source": [
        "# 모형 학습\r\n",
        "\r\n",
        "# 오차함수 객체\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# 최적화 담당\r\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)  # 확률적경사하강법은 SGD (데이터의 미니배치를 사용하여 경사하강법을 구현)\r\n",
        "\r\n",
        "# 학습시작\r\n",
        "for epoch in range(1000):\r\n",
        "  total_loss = 0\r\n",
        "\r\n",
        "  for train_x, train_y in train_loader:\r\n",
        "    train_x, train_y = Variable(train_x), Variable(train_y)  # 미분을 가능하게 : \".backward()\"에 사용하는 변수인데 사용안해도 되던데.... 아리송송\r\n",
        "\r\n",
        "    optimizer.zero_grad()  # 경사초기화\r\n",
        "\r\n",
        "    output = model(train_x)  # 순전파 계산\r\n",
        "\r\n",
        "    loss = criterion(output, train_y)  # 오차 계산\r\n",
        "    loss.backward()  # 역전파 계산\r\n",
        "\r\n",
        "    optimizer.step()  #가중치 업데이트\r\n",
        "\r\n",
        "    total_loss += loss.data  # 누적오차 계산\r\n",
        "  \r\n",
        "  if (epoch+1) % 100 == 0:\r\n",
        "    print(epoch+1, float(total_loss))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100 46.75200271606445\n",
            "200 3.4959702491760254\n",
            "300 0.7158715724945068\n",
            "400 0.2541388273239136\n",
            "500 0.1834527850151062\n",
            "600 0.09309367090463638\n",
            "700 0.08949225395917892\n",
            "800 0.047558464109897614\n",
            "900 0.05242578685283661\n",
            "1000 0.04234261438250542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zz8EssdyONW",
        "outputId": "31fb1458-1718-4ee7-829b-1e4ab0c473e2"
      },
      "source": [
        "test_x, test_y = Variable(test_x), Variable(test_y)   # 이거 안해도 문제 없움\r\n",
        "\r\n",
        "# 출력이 0또는 1이 되도록\r\n",
        "result = torch.max(model(test_x).data.cpu(), 1)[1]  # 모델이 분류하는 거에 따라 측정값이 달라지네\r\n",
        "\r\n",
        "# 측정\r\n",
        "accuracy = sum(test_y.data.cpu().numpy() == result.numpy()) / len(test_y.data.cpu().numpy())\r\n",
        "accuracy\r\n",
        "\r\n",
        "# torch.max()는 최댓값을 반환하여 값을 매기는데, 이게 두개의 열이라면 첫 열이 크면 0을 나타내고, 두번째 열이 크면 1을 나타냄(그 위치?를 나타낸다고 볼 수 있우)\r\n",
        "# .cpu()뒤에 .detach를 붙이나 안붙이나 차이가 왜 없는 거 같냐..."
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.906"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMgcDxjfrCD6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_g0SnQqrCBY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEVepUe53BFR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIgCaWg23BC6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1KC2tFb3BAR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Cxff0h8htH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwah1U_i8hqH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h16zkU0b8hnT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}